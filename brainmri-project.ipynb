{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-21T18:11:56.518372Z","iopub.execute_input":"2022-08-21T18:11:56.518958Z","iopub.status.idle":"2022-08-21T18:12:04.671632Z","shell.execute_reply.started":"2022-08-21T18:11:56.518923Z","shell.execute_reply":"2022-08-21T18:12:04.670672Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# IMPORTING DATASET","metadata":{}},{"cell_type":"code","source":"kaggle datasets download -d mateuszbuda/lgg-mri-segmentation","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:12:04.673616Z","iopub.execute_input":"2022-08-21T18:12:04.673972Z","iopub.status.idle":"2022-08-21T18:12:04.682787Z","shell.execute_reply.started":"2022-08-21T18:12:04.673935Z","shell.execute_reply":"2022-08-21T18:12:04.679978Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# IMPORTING DEPENDENCIES","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n%matplotlib inline\n\nimport cv2\nfrom tqdm import tqdm_notebook, tnrange\nfrom glob import glob\nfrom itertools import chain\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom skimage.color import rgb2gray\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model, load_model, save_model\nfrom tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:12:40.543880Z","iopub.execute_input":"2022-08-21T18:12:40.544279Z","iopub.status.idle":"2022-08-21T18:12:46.590768Z","shell.execute_reply.started":"2022-08-21T18:12:40.544243Z","shell.execute_reply":"2022-08-21T18:12:46.589767Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# **DATA PREPROCESSING STAGE**","metadata":{}},{"cell_type":"code","source":"# Setting size parameters of images\nim_width = 256\nim_height = 256 #This can be changed to 512 afterwards for quality enhancement","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:12:55.379389Z","iopub.execute_input":"2022-08-21T18:12:55.380259Z","iopub.status.idle":"2022-08-21T18:12:55.385079Z","shell.execute_reply.started":"2022-08-21T18:12:55.380220Z","shell.execute_reply":"2022-08-21T18:12:55.383991Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"image_filenames_train = []\n\nmask_files = glob('../input/lgg-mri-segmentation/kaggle_3m/*/*_mask*')\n\nfor i in mask_files:\n    image_filenames_train.append(i.replace('_mask', ''))\n\nprint(image_filenames_train[:10])\nlen(image_filenames_train)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:12:57.662675Z","iopub.execute_input":"2022-08-21T18:12:57.663740Z","iopub.status.idle":"2022-08-21T18:12:57.771095Z","shell.execute_reply.started":"2022-08-21T18:12:57.663693Z","shell.execute_reply":"2022-08-21T18:12:57.769980Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# FUNCTIONS ","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tensorflow.keras import backend as K\n\nplt.style.use(\"ggplot\")\n\n\ndef plot_from_img_path(rows, columns, list_img_path, list_mask_path):\n    fig = plt.figure(figsize=(12, 12))\n    for i in range(1, rows * columns + 1):\n        fig.add_subplot(rows, columns, i)\n        img_path = list_img_path[i]\n        mask_path = list_mask_path[i]\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path)\n        plt.imshow(image)\n        plt.imshow(mask, alpha=0.4)\n    plt.show()\n\n\ndef dice_coefficients(y_true, y_pred, smooth=100): # ((2 * the area of overlap) / total number of pixels in both the images)  compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth.\n    y_true_flatten = K.flatten(y_true) #Ground Truth\n    y_pred_flatten = K.flatten(y_pred) #Predicted Value\n\n    #When dice_coefficient is high, then it means High Accuracy in Prediction\n    \n    intersection = K.sum(y_true_flatten * y_pred_flatten)\n    union = K.sum(y_true_flatten) + K.sum(y_pred_flatten)\n    return (2 * intersection + smooth) / (union + smooth)\n\n\ndef dice_coefficients_loss(y_true, y_pred, smooth=100): \n    return -dice_coefficients(y_true, y_pred, smooth)\n\n\ndef iou(y_true, y_pred, smooth=100): #Finding Intersection over Union\n    intersection = K.sum(y_true * y_pred)\n    sum = K.sum(y_true + y_pred)\n    iou = (intersection + smooth) / (sum - intersection + smooth)\n    return iou\n\n\ndef jaccard_distance(y_true, y_pred):\n    y_true_flatten = K.flatten(y_true)\n    y_pred_flatten = K.flatten(y_pred)\n    return -iou(y_true_flatten, y_pred_flatten)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:13:01.458819Z","iopub.execute_input":"2022-08-21T18:13:01.460674Z","iopub.status.idle":"2022-08-21T18:13:01.481728Z","shell.execute_reply.started":"2022-08-21T18:13:01.460620Z","shell.execute_reply":"2022-08-21T18:13:01.480530Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# PLOT MASK AND DATASET IMAGES RANDOMLY","metadata":{}},{"cell_type":"code","source":"plot_from_img_path(3, 3 , image_filenames_train, mask_files )","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:13:06.097556Z","iopub.execute_input":"2022-08-21T18:13:06.098559Z","iopub.status.idle":"2022-08-21T18:13:07.661306Z","shell.execute_reply.started":"2022-08-21T18:13:06.098512Z","shell.execute_reply":"2022-08-21T18:13:07.660297Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# TRAIN, TEST and VALIDATION DATASET SPLIT","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(data={'image_filenames_train': image_filenames_train, 'mask': mask_files })\n\n#Split the main dataset into training dataset and test dataset\ndf_train, df_test = train_test_split(df, test_size=0.1) #90% images = train ds and 10% = test ds\n\n# Further split training dataset into validation dataset and training dataset\ndf_train, df_val = train_test_split(df_train, test_size=0.2) #out of 90% of train ds, 20% to validation ds and remaining 80% to training ds\n\nprint(df_train.shape)\nprint(df_test.shape)\nprint(df_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:13:13.957978Z","iopub.execute_input":"2022-08-21T18:13:13.958561Z","iopub.status.idle":"2022-08-21T18:13:13.980645Z","shell.execute_reply.started":"2022-08-21T18:13:13.958516Z","shell.execute_reply":"2022-08-21T18:13:13.979708Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#To pass data to model, batch by batch, Data Generatot is required\n\n# Referring Code From: https://github.com/zhixuhao/unet/blob/master/data.py\ndef train_generator(\n    data_frame,\n    batch_size,\n    augmentation_dict,\n    image_color_mode=\"rgb\",\n    mask_color_mode=\"grayscale\",\n    image_save_prefix=\"image\",\n    mask_save_prefix=\"mask\",\n    save_to_dir=None,\n    target_size=(256, 256),\n    seed=1,\n):\n    \"\"\"\n    It can generate image and mask at the same time, Use the same seed for\n    image_datagen and mask_datagen to ensure the transformation for image\n    and mask is the same.\n    # If you want to visualize the results of generator,\n    set save_to_dir = \"your path\"\n    \"\"\"\n    image_datagen = ImageDataGenerator(**augmentation_dict)\n    mask_datagen = ImageDataGenerator(**augmentation_dict)\n\n    image_generator = image_datagen.flow_from_dataframe(\n        data_frame,\n        x_col=\"image_filenames_train\",\n        class_mode=None, #default is categorical, you may set it to binary_class_entropy\n        color_mode=image_color_mode, #default: rgb, you may set it to rgba\n        target_size=target_size, #Height and Width, currently 256*256\n        batch_size=batch_size,\n        save_to_dir=save_to_dir,\n        save_prefix=image_save_prefix,\n        seed=seed,\n    )\n\n    mask_generator = mask_datagen.flow_from_dataframe(\n        data_frame,\n        x_col=\"mask\",\n        class_mode=None,\n        color_mode=mask_color_mode,\n        target_size=target_size,\n        batch_size=batch_size,\n        save_to_dir=save_to_dir,\n        save_prefix=mask_save_prefix,\n        seed=seed,\n    )\n\n    train_gen = zip(image_generator, mask_generator)\n    \n    # Final return Tuple after image Normalization and Diagnostics\n    for (img, mask) in train_gen:\n        img, mask = normalize_and_diagnose(img, mask)\n        yield (img, mask)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:13:17.587631Z","iopub.execute_input":"2022-08-21T18:13:17.588017Z","iopub.status.idle":"2022-08-21T18:13:17.597211Z","shell.execute_reply.started":"2022-08-21T18:13:17.587984Z","shell.execute_reply":"2022-08-21T18:13:17.596212Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"''' After mask Normalization if the value is <= 0.5 then that Mask \nwill be considered a complete black one and does not have any Tumor '''\ndef normalize_and_diagnose(img, mask):\n    img = img / 255\n    mask = mask / 255\n    mask[mask > 0.5] = 1 #Classify this mask as Tumor\n    mask[mask <= 0.5] = 0 #Classify this mask as Non-Tumor\n    return(img, mask)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:13:22.263466Z","iopub.execute_input":"2022-08-21T18:13:22.264034Z","iopub.status.idle":"2022-08-21T18:13:22.271266Z","shell.execute_reply.started":"2022-08-21T18:13:22.263990Z","shell.execute_reply":"2022-08-21T18:13:22.270291Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Set some Hyperparameters","metadata":{}},{"cell_type":"code","source":"EPOCHS = 100\nBATCH_SIZE = 32\nlearning_rate = 1e-4\nsmooth=100","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:13:25.837433Z","iopub.execute_input":"2022-08-21T18:13:25.838029Z","iopub.status.idle":"2022-08-21T18:13:25.843176Z","shell.execute_reply.started":"2022-08-21T18:13:25.837996Z","shell.execute_reply":"2022-08-21T18:13:25.842236Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# MODEL","metadata":{}},{"cell_type":"code","source":"#UNET model is a combination of Encoder and Decoder, joined by skip connections b/w them\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model, load_model, save_model\nfrom tensorflow.keras.layers import (\n    Input,\n    Activation,\n    BatchNormalization,\n    Dropout,\n    Lambda,\n    Conv2D,\n    Conv2DTranspose,\n    MaxPooling2D,\n    concatenate,\n)\nfrom tensorflow.keras import backend as K\n\n\ndef unet(input_size=(256, 256, 3)):\n    inputs = Input(input_size)\n\n    # First DownConvolution / Encoder Leg will begin, so start with Conv2D\n    conv1 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(inputs)\n    bn1 = Activation(\"relu\")(conv1)\n    conv1 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(bn1)\n    bn1 = BatchNormalization(axis=3)(conv1)\n    bn1 = Activation(\"relu\")(bn1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(bn1)\n\n    conv2 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(pool1)\n    bn2 = Activation(\"relu\")(conv2)\n    conv2 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(bn2)\n    bn2 = BatchNormalization(axis=3)(conv2)\n    bn2 = Activation(\"relu\")(bn2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(bn2)\n\n    conv3 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(pool2)\n    bn3 = Activation(\"relu\")(conv3)\n    conv3 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(bn3)\n    bn3 = BatchNormalization(axis=3)(conv3)\n    bn3 = Activation(\"relu\")(bn3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(bn3)\n\n    conv4 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(pool3)\n    bn4 = Activation(\"relu\")(conv4)\n    conv4 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(bn4)\n    bn4 = BatchNormalization(axis=3)(conv4)\n    bn4 = Activation(\"relu\")(bn4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(bn4)\n\n    conv5 = Conv2D(filters=1024, kernel_size=(3, 3), padding=\"same\")(pool4) #Acts as an intermediate layer b/w encoder and decoder\n    bn5 = Activation(\"relu\")(conv5)\n    conv5 = Conv2D(filters=1024, kernel_size=(3, 3), padding=\"same\")(bn5)\n    bn5 = BatchNormalization(axis=3)(conv5)\n    bn5 = Activation(\"relu\")(bn5)\n\n    \"\"\" Now UpConvolution / Decoder Leg will begin, so start with Conv2DTranspose\n     The skip connections that concatenate the encoder feature map with the decoder, which helps the backward flow of gradients for improved training. \"\"\"\n    up6 = concatenate(\n        [\n            Conv2DTranspose(512, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(\n                bn5\n            ),\n            conv4,\n        ],\n        axis=3,\n    )\n    \"\"\" After every concatenation we again apply two consecutive regular convolutions so that the model can learn to assemble a more precise output \"\"\"\n    conv6 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(up6) #conv_6 integrated with skip connection from conv_4 \n    bn6 = Activation(\"relu\")(conv6)\n    conv6 = Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\")(bn6)\n    bn6 = BatchNormalization(axis=3)(conv6)\n    bn6 = Activation(\"relu\")(bn6)\n    \n    up7 = concatenate(\n        [\n            Conv2DTranspose(256, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(\n                bn6\n            ),\n            conv3,\n        ],\n        axis=3,\n    )\n    conv7 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(up7) #conv_7 integrated with skip connection from conv_3 \n    bn7 = Activation(\"relu\")(conv7)\n    conv7 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(bn7)\n    bn7 = BatchNormalization(axis=3)(conv7)\n    bn7 = Activation(\"relu\")(bn7)\n\n    up8 = concatenate(\n        [\n            Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(\n                bn7\n            ),\n            conv2,\n        ],\n        axis=3,\n    )\n    conv8 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(up8) #conv_8 integrated with skip connection from conv_2 \n    bn8 = Activation(\"relu\")(conv8)\n    conv8 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(bn8)\n    bn8 = BatchNormalization(axis=3)(conv8)\n    bn8 = Activation(\"relu\")(bn8)\n\n    up9 = concatenate(\n        [\n            Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(\n                bn8\n            ),\n            conv1,\n        ],\n        axis=3,\n    )\n    \n    conv9 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(up9) #conv_9 integrated with skip connection from conv_1 \n    bn9 = Activation(\"relu\")(conv9)\n    conv9 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(bn9)\n    bn9 = BatchNormalization(axis=3)(conv9)\n    bn9 = Activation(\"relu\")(bn9)\n\n    conv10 = Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\")(bn9)\n\n    return Model(inputs=[inputs], outputs=[conv10])","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:13:29.595158Z","iopub.execute_input":"2022-08-21T18:13:29.595695Z","iopub.status.idle":"2022-08-21T18:13:29.619344Z","shell.execute_reply.started":"2022-08-21T18:13:29.595660Z","shell.execute_reply":"2022-08-21T18:13:29.618083Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\nmodel = unet()\nmodel.summary","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:13:34.917868Z","iopub.execute_input":"2022-08-21T18:13:34.918338Z","iopub.status.idle":"2022-08-21T18:13:38.439102Z","shell.execute_reply.started":"2022-08-21T18:13:34.918296Z","shell.execute_reply":"2022-08-21T18:13:38.438183Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# DATA AUGMENTATION AND TRAIN THE MODEL ON AUGMENTED DATA","metadata":{}},{"cell_type":"code","source":"train_generator_param = dict(rotation_range=0.2,\n                            width_shift_range=0.05,\n                            height_shift_range=0.05,\n                            shear_range=0.05,\n                            zoom_range=0.05,\n                            horizontal_flip=True,\n                            fill_mode='nearest')\n\ntrain_gen = train_generator(df_train, BATCH_SIZE,\n                                train_generator_param,\n                                target_size=(im_height, im_width))\n    \ntest_gen = train_generator(df_val, BATCH_SIZE,\n                                dict(),\n                                target_size=(im_height, im_width))\n    \nmodel = unet(input_size=(im_height, im_width, 3))\n\n\n\ndecay_rate = learning_rate / EPOCHS\n\nopt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)\n\nmodel.compile(optimizer=opt, loss=dice_coefficients_loss, metrics=[\"binary_accuracy\", iou, dice_coefficients])\n\ncallbacks = [ModelCheckpoint('unet.hdf5', verbose=1, save_best_only=True)]\n\nhistory = model.fit(train_gen,\n                    steps_per_epoch=len(df_train) / BATCH_SIZE, \n                    epochs=EPOCHS, \n                    callbacks=callbacks,\n                    validation_data = test_gen,\n                    validation_steps=len(df_val) / BATCH_SIZE)\n\n#Lower the dice coefficient loss, higher the accuracy","metadata":{"execution":{"iopub.status.busy":"2022-08-21T18:13:49.343427Z","iopub.execute_input":"2022-08-21T18:13:49.343991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plots: Accuracy, Loss","metadata":{}},{"cell_type":"code","source":"history_post_training = history.history\n\ntrain_dice_coeff_list = history_post_training['dice_coefficient']\ntest_dice_coeff_list = history_post_training['val_dice_coefficient']\n\ntrain_jaccard_list = history_post_training['iou']\ntest_jaccard_list = history_post_training['val_iou']\n\ntrain_loss_list = history_post_training['loss']\ntest_loss_list = history_post_training['val_loss']\n\nplt.figure(1)\nplt.plot(test_loss_list, 'b-')\nplt.plot(train_loss_list, 'r-')\n\nplt.xlabel('iterations')\nplt.ylabel('loss')\nplt.title('loss graph', fontsize=12)\n\nplt.figure(2)\nplt.plot(train_dice_coeff_list, 'b-')\nplt.plot(test_dice_coeff_list, 'r-')\n\nplt.xlabel('iterations')\nplt.ylabel('accuracy')\nplt.title('Accuracy graph', fontsize=12)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}